{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Y57umZKqIG",
        "outputId": "42f05eae-392c-4649-b5ed-676609f77743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " age               0\n",
            "job               0\n",
            "marital           0\n",
            "education         0\n",
            "default           0\n",
            "housing           0\n",
            "loan              0\n",
            "contact           0\n",
            "month             0\n",
            "day_of_week       0\n",
            "duration          0\n",
            "campaign          0\n",
            "pdays             0\n",
            "previous          0\n",
            "poutcome          0\n",
            "emp.var.rate      0\n",
            "cons.price.idx    0\n",
            "cons.conf.idx     0\n",
            "euribor3m         0\n",
            "nr.employed       0\n",
            "y                 0\n",
            "dtype: int64\n",
            "\n",
            "Data Types:\n",
            " age                 int64\n",
            "job                object\n",
            "marital            object\n",
            "education          object\n",
            "default            object\n",
            "housing            object\n",
            "loan               object\n",
            "contact            object\n",
            "month              object\n",
            "day_of_week        object\n",
            "duration            int64\n",
            "campaign            int64\n",
            "pdays               int64\n",
            "previous            int64\n",
            "poutcome           object\n",
            "emp.var.rate      float64\n",
            "cons.price.idx    float64\n",
            "cons.conf.idx     float64\n",
            "euribor3m         float64\n",
            "nr.employed       float64\n",
            "y                  object\n",
            "dtype: object\n",
            "\n",
            "Class Distribution:\n",
            " y\n",
            "0    3668\n",
            "1     451\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Weights:\n",
            " {0: 0.5614776444929117, 1: 4.566518847006652}\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/bank-additional.csv'\n",
        "data = pd.read_csv(file_path, sep=';')\n",
        "\n",
        "# ---------------------------------\n",
        "# Step 1: Data Cleaning\n",
        "# ---------------------------------\n",
        "# Check for missing values and fill if necessary\n",
        "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
        "if data.isnull().sum().sum() > 0:\n",
        "    print(\"Handling missing values...\")\n",
        "    data.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
        "\n",
        "# Check and ensure data types are consistent\n",
        "print(\"\\nData Types:\\n\", data.dtypes)\n",
        "\n",
        "# ---------------------------------\n",
        "# Step 2: Transformation\n",
        "# ---------------------------------\n",
        "# Separate categorical and numerical features\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns.drop('y')  # Exclude the target column 'y'\n",
        "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_features = encoder.fit_transform(data[categorical_columns])\n",
        "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoded_columns)\n",
        "\n",
        "# Combine numerical and encoded categorical features\n",
        "processed_features = pd.concat([data[numerical_columns], encoded_df], axis=1)\n",
        "\n",
        "# Encode target variable 'y' as binary (1 for 'yes', 0 for 'no')\n",
        "processed_features['y'] = data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
        "\n",
        "# ---------------------------------\n",
        "# Step 3: Handle Class Imbalance\n",
        "# ---------------------------------\n",
        "# Check for class distribution\n",
        "class_distribution = processed_features['y'].value_counts()\n",
        "print(\"\\nClass Distribution:\\n\", class_distribution)\n",
        "\n",
        "# Calculate class weights for the target variable\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(processed_features['y']),\n",
        "    y=processed_features['y']\n",
        ")\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(\"\\nClass Weights:\\n\", class_weight_dict)\n",
        "\n",
        "# ---------------------------------\n",
        "# Step 4: Feature Scaling\n",
        "# ---------------------------------\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_data = processed_features[numerical_columns]\n",
        "scaled_numerical_data = scaler.fit_transform(numerical_data)\n",
        "\n",
        "# Replace numerical features with scaled versions\n",
        "processed_features[numerical_columns] = scaled_numerical_data\n",
        "\n",
        "# ---------------------------------\n",
        "\n",
        "# Step 5: Save Preprocessed Data\n",
        "# ---------------------------------\n",
        "# Save the preprocessed dataset to a CSV file\n",
        "processed_features.to_csv('preprocessed_bank_data.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYCCWnK_KxJJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}